---
title: "Lab 08 - Text Mining"
output: html_document
---

```{r setup}
knitr::opts_chunk$set(eval = F, include  = T)
```

# Learning goals

- Use `unnest_tokens()` and `unnest_ngrams()` to extract tokens and ngrams from text.
- Use dplyr and ggplot2 to analyze text data

# Lab description

For this lab we will be working with the medical record transcriptions from https://www.mtsamples.com/. And is loaded and "fairly" cleaned at https://github.com/JSC370/jsc370-2022/blob/main/data/medical_transcriptions/mtsamples.csv.

This markdown document should be rendered using `github_document` document.



### Setup packages

You should load in `dplyr`, (or `data.table` if you want to work that way), `ggplot2` and `tidytext`.
If you don't already have `tidytext` then you can install with

```{r, eval=FALSE}
install.packages("tidytext")
```

### read in Medical Transcriptions

Loading in reference transcription samples from https://www.mtsamples.com/

```{r, warning=FALSE, message=FALSE}
library(tidytext)
library(readr)
library(dplyr)
library(tidyr)
library(ggplot2)
library(wordcloud)

mt_samples <- read_csv("https://raw.githubusercontent.com/JSC370/jsc370-2022/main/data/medical_transcriptions/mtsamples.csv")
mt_samples <- mt_samples %>%
  select(description, medical_specialty, transcription)

head(mt_samples)
```

---

## Question 1: What specialties do we have?

We can use `count()` from `dplyr` to figure out how many different catagories do we have? Are these catagories related? overlapping? evenly distributed?

```{r}
mt_samples %>%
  count(medical_specialty, sort = TRUE) %>%
  ggplot(aes(medical_specialty, n)) +geom_col() +coord_flip()
```

The categories are related- they are different areas of med. They are not evenly distributed- we can see that surgery has the largest count by far. They are not overlapping.
---

## Question 2

- Tokenize the the words in the `transcription` column
- Count the number of times each token appears
- Visualize the top 20 most frequent words

Explain what we see from this result. Does it makes sense? What insights (if any) do we get?

```{r}
tokens <- mt_samples %>%
  select(transcription) %>%
  unnest_tokens(word, transcription) %>%
  group_by(word) %>%
  summarise(word_frequency =n()) %>%
  arrange(across(word_frequency, desc)) %>%
  head(20)

tokens %>%
  ggplot(aes(reorder(word, word_frequency), word_frequency)) +
  geom_bar(stat = "identity") +coord_flip()

library(wordcloud)
wordcloud(tokens$word, tokens$word_frequency)
```

The results show us that stop words such as "the" and "and" occur very frequent. This is to be expected as these words are needed in virtually every sentence in order for the sentence to make sense. We do not get much insight from this.

---

## Question 3

- Redo visualization but remove stopwords before
- Bonus points if you remove numbers as well

What do we see know that we have removed stop words? Does it give us a better idea of what the text is about?

```{r}
tokens <- mt_samples %>%
  select(transcription) %>%
  unnest_tokens(word, transcription) %>%
  anti_join(stop_words, by="word") %>%
  subset(!grepl("^\\d+$", word)) %>%
  group_by(word) %>%
  summarise(word_frequency = n()) %>%
  arrange(across(word_frequency, desc)) %>%
  head(20)

tokens %>%
  ggplot(aes(word, word_frequency)) +
  geom_bar(stat="identity") + coord_flip()

wordcloud(tokens$word, tokens$word_frequency)
```

After removing the stop words, we see that words that contribute to the meaning of the sentence. The word "patient" is the most frequently used. The word "left" is the next most frequent, followed by "history".

---

# Question 4

Repeat question 2, but this time tokenize into bi-grams. How does the result change if you look at tri-grams?
```{r}
tokens <- mt_samples %>%
  select(transcription) %>%
  unnest_tokens(bigram, transcription, token="ngrams", n=2) %>%
  group_by(bigram) %>%
  summarise(bigram_frequency=n()) %>%
  
  separate(bigram, c("word1", "word2"), extra="drop", remove=F, sep=" ", fill="right")

tokens %>%
  anti_join(stop_words, by=c("word1" = "word")) %>%
  anti_join(stop_words, by=c("word2" = "word")) %>%
  subset(!grepl("\\d+", bigram)) %>%
  arrange(across(bigram_frequency, desc)) %>%
  head(20) %>%
  
  ggplot(aes(bigram, bigram_frequency)) +
  geom_bar(stat = "identity") + coord_flip()
```


```{r}

tokens <- mt_samples %>%
  select(transcription) %>%
  unnest_tokens(trigram, transcription, token="ngrams", n=3) %>%
  group_by(trigram) %>%
  summarise(trigram_frequency = n()) %>% 
  separate(trigram, c("word1", "word2", "word3"),
                              extra="drop", remove=F, sep=" ", fill="right")

tokens %>%
  anti_join(stop_words, by=c("word1" = "word")) %>%
  anti_join(stop_words, by=c("word2" = "word")) %>%
  anti_join(stop_words, by=c("word3" = "word")) %>%
  subset(!grepl("\\d+", trigram)) %>%
  arrange(across(trigram_frequency, desc)) %>%
  head(20) %>%
  
  ggplot(aes(trigram, trigram_frequency)) +
  geom_bar(stat = "identity") + coord_flip()


```

The bigram plot takes a look at phrases with 2 words; while the trigram plot takes a lot at phrases with 3 words. With bigrams, the phrase with the highest frequency is "blood pressure", followed by "medical history". For trigrams, we see that the phrase with the highest frequency is "past medical history", followed by "estimated blood loss". For trigram, phrases with "blood pressure" in it no longer have the top frequencies- unlike for bigrams.

---

# Question 5

Using the results you got from question 4. Pick a word and count the words that appears after and before it.
```{r}
tokens %>%
  subset(word2=="blood") %>%
  group_by(word1) %>%
  summarise(word1_freq = n()) %>%
  arrange(across(word1_freq, desc)) %>%
  head(20) %>%
  ggplot(aes(word1, word1_freq)) +
  geom_bar(stat="identity") + coord_flip()

tokens %>%
  subset(word2=="blood") %>%
  group_by(word3) %>%
  summarise(word3_freq = n()) %>%
  arrange(across(word3_freq, desc)) %>%
  head(20) %>%
  ggplot(aes(word3, word3_freq)) +
  geom_bar(stat="identity") + coord_flip()
```

The word that was picked was "blood".

---

# Question 6 

Which words are most used in each of the specialties. you can use `group_by()` and `top_n()` from `dplyr` to have the calculations be done within each specialty. Remember to remove stopwords. How about the most 5 used words?

```{r}
mt_samples %>%
  unnest_tokens(word, transcription) %>%
  anti_join(stop_words, by="word") %>%
  subset(!grepl("^\\d+$", word)) %>%
  group_by(medical_specialty) %>%
  count(word, sort=T) %>%
  top_n(1, n)

mt_samples %>%
  unnest_tokens(word, transcription) %>%
  anti_join(stop_words, by="word") %>%
  subset(!grepl("^\\d+$", word)) %>%
  group_by(medical_specialty) %>%
  count(word, sort=T) %>%
  top_n(5, n)
```
The word most used by the cateogry surgery was "patient" with a count of 4855. This word is also the most used word for many other specialties such as orthopedic and general medicine.

# Question 7 - extra

Find your own insight in the data:

Ideas:

- Interesting ngrams
- See if certain words are used more in some specialties then others

Here, I choose to work with a 4-gram:

```{r}

tokens <- mt_samples %>%
  select(transcription) %>%
  unnest_tokens(quadgram, transcription, token="ngrams", n=4) %>%
  group_by(quadgram) %>%
  summarise(quadgram_frequency = n()) %>% 
  separate(quadgram, c("word1", "word2", "word3", "word4"),
                              extra="drop", remove=F, sep=" ", fill="right")

tokens %>%
  anti_join(stop_words, by=c("word1" = "word")) %>%
  anti_join(stop_words, by=c("word2" = "word")) %>%
  anti_join(stop_words, by=c("word3" = "word")) %>%
  anti_join(stop_words, by=c("word4" = "word")) %>%
  subset(!grepl("\\d+", quadgram)) %>%
  arrange(across(quadgram_frequency, desc)) %>%
  head(20) %>%
  
  ggplot(aes(quadgram, quadgram_frequency)) +
  geom_bar(stat = "identity") + coord_flip()


```

# Deliverables

1. Questions 1-7 answered, pdf or html output uploaded to quercus
