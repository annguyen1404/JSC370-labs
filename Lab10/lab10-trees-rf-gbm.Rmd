---
title: "Lab 10 - Trees, Bagging, Random Forest"
output: html_document
---

```{r setup}
knitr::opts_chunk$set(eval = F, include  = T)
```

# Learning goals

- Perform classification and regression with tree-based methods in R
- Recognize that tree-based methods are capable of capturing non-linearities by splitting multiple times on the same variables
- Compare the performance of classification trees, bagging,and random forests for predicting heart disease based on the ``heart`` data.

# Lab description

For this lab we will be working with simulated data and the `heart` dataset that you can download from [here](https://github.com/JSC370/jsc370-2022/blob/main/data/heart/heart.csv)


### Setup packages

You should install and load `rpart` (trees), `randomForest` (random forest), `gbm` (gradient boosting) and `xgboost` (extreme gradient boosting).


```{r, eval=FALSE, warning=FALSE}
install.packages(c("rpart","randomForest","gbm","xgboost"))
```

### Load packages and data
```{r, eval=TRUE, warning=FALSE, message=FALSE, echo=FALSE, warning=FALSE}
library(tidyverse)
library(rpart)
library(rpart.plot)
library(randomForest)
library(gbm)
library(xgboost)
library(ggplot2)

heart<-read.csv("https://raw.githubusercontent.com/JSC370/jsc370-2022/main/data/heart/heart.csv")

head(heart)
```


---

## Question 1: Trees with simulated data

- Simulate data from a random uniform distribution [-5,5] and normally distributed errors (s.d = 0.5)
- Create a non-linear relationship y=sin(x)+error
- Split the data into test and training sets (500 points each), plot the data

```{r, eval=TRUE, echo=FALSE, warning=FALSE}
set.seed(123)

n <- 1000
x <- runif(n, -5, 5)
error <- rnorm(n, sd=0.5)
y <- sin(x)+error
nonlin <- data.frame(y=y, x=x)

train_size <- sample(1:1000, size=500)
nonlin_train <- nonlin[train_size,]
nonlin_test <- nonlin[-train_size,]

ggplot(nonlin, aes(y=y, x=x))+
  geom_point()

```

- Fit a regression tree using the training set, plot it

```{r, eval=TRUE, echo=FALSE, warning=FALSE}
treefit <- rpart(y~x, method="anova", control=list(cp=0), data=nonlin_train)
rpart.plot(treefit)

```

- Determine the optimal complexity parameter (cp) to prune the tree

```{r, eval=TRUE, echo=FALSE, warning=FALSE}

plotcp(treefit)
printcp(treefit)

# Looking for the row with the min xerror
optimalcp<- 0.00287664 

```

- Prune and plot the tree and summarize

```{r, eval=TRUE, echo=FALSE, warning=FALSE}

treepruned <- prune(treefit, cp=optimalcp)
summary(treepruned)

```

- Based on the plot and/or summary of the pruned tree create a vector of the (ordered) split points for variable x, and a vector of fitted values for the intervals determined by the split points of x.

```{r, echo=FALSE, eval=TRUE, warning=FALSE}

x_splits <- sort(treepruned$splits[,'index'])
y_splits <- treepruned$frame[which(treepruned$frame[,'var']=="<leaf>"), 'yval']


```
- plot the step function corresponding to the fitted (pruned) tree
```{r, eval=TRUE, echo=FALSE, warning=FALSE}

plot(y~x, data=nonlin_train)
plot(stepfun(x_splits, y_splits), add=T, col='red')

```

- Fit a linear model to the training data and plot the regression line. 
- Contrast the quality of the fit of the tree model vs. linear regression by inspection of the plot
- Compute the test MSE of the pruned tree and the linear regression model

```{r, echo=FALSE, eval=TRUE, warning=FALSE}

lmfit <- lm(y~x, data=nonlin_train)
summary(lmfit)

plot(y~x)
abline(lmfit, col="blue")
plot(stepfun(x_splits, y_splits), add=T, col='red')

tree_pred <- predict(treepruned, nonlin_test)
nonlin_test_tree <- cbind(nonlin_test, tree_pred)

tree_mse <- sum((nonlin_test_tree$tree_pred - nonlin_test_tree$y)^2)/dim(nonlin_test_tree)[1]
tree_mse

lm_pred <- predict(lmfit, nonlin_test)
nonlin_test_lm <- cbind(nonlin_test, lm_pred)
lm_mse <- sum((nonlin_test_lm$lm_pred - nonlin_test_lm$y)^2)/dim(nonlin_test_tree)[1]
lm_mse
```
- Is the lm or regression tree better at fitting a non-linear function?
The regression tree MSE is less than that of lm's and the regression tree fits the non-linear function better.

---

## Question 2: Analysis of Real Data

- Split the `heart` data into training and testing (70-30%)
```{r, echo=FALSE, eval=TRUE, warning=FALSE}

set.seed(1223)
train_idx <- sample(1:nrow(heart), round(0.7*nrow(heart)))
train <- heart[train_idx,]
test <- heart[-train_idx,]

```

- Fit a classification tree using rpart, plot the full tree
```{r, echo=FALSE, eval=TRUE, warning=FALSE}
heart_tree <- rpart(AHD~., data=train, method="class", control=list(minsplit=10, minbucket=3, cp=0, xval=10))

rpart.plot(heart_tree)
```

- Plot the complexity parameter table for an rpart fit and prune the tree
```{r, echo=FALSE, eval=TRUE, warning=FALSE}

plotcp(heart_tree)
printcp(heart_tree)

optimalcp <- heart_tree$cptable[which.min(heart_tree$cptable[,"xerror"]), "CP"]
optimalcp

heart_tree_prune <- prune(heart_tree, cp=optimalcp)
rpart.plot(heart_tree_prune)

```

```{r, echo=FALSE, eval=TRUE, warning=FALSE}
heart_pred <- predict(heart_tree_prune, test)
heart_pred <- as.data.frame(heart_pred)
heart_pred$AHD <- ifelse(heart_pred$Yes>0.5, "yes", "no")

confmatrix_table <- table(true=test$AHD, predicted=heart_pred$AHD)
confmatrix_table

```

- Compute the test misclassification error
```{r, echo=FALSE, eval=TRUE, warning=FALSE}

misclass_err <-(confmatrix_table[1, 2] + confmatrix_table[2, 1])/nrow(test)
misclass_err

```

- Fit the tree with the optimal complexity parameter to the full data (training + testing)
```{r,echo=FALSE, eval=TRUE, warning=FALSE}

heart_tree <- rpart(AHD~., data=heart, method="class", control=list(cp=optimalcp))
plotcp(heart_tree)

```
 - Out of Bag (OOB) error for tree
 
```{r,echo=FALSE, eval=TRUE, warning=FALSE}

heart_tree$cptable
min(heart_tree$cptable[, "xerror"])*nrow(heart)

```

---

## Question 3: Bagging, Random Forest

- Use the training and testing sets from above. Train each of the models on the training data and extract the cross-validation (or out-of-bag error for bagging and Random forest). 
- For bagging use ``randomForest`` with ``mtry`` equal to the number of features (all other parameters at their default values). Generate the variable importance plot using ``varImpPlot`` and extract variable importance from the ``randomForest`` fitted object using the ``importance`` function.

```{r, echo=FALSE, eval=TRUE, warning=FALSE}

heart_bag <- randomForest(as.factor(AHD)~., data=train, mtry=13, na.action=na.omit)
sum(heart_bag$err.rate[, 1])
varImpPlot(heart_bag, n.var=13, col="red")
importance(heart_bag)

```

- For random forests use ``randomForest`` with the default parameters. Generate the variable importance plot using ``varImpPlot`` and extract variable importance from the ``randomForest`` fitted object using the ``importance`` function.

```{r, echo=FALSE, eval=TRUE, warning=FALSE}

heart_rf <- randomForest(as.factor(AHD)~., data=train, na.action=na.omit)
sum(heart_rf$err.rate[, 1])
varImpPlot(heart_rf, n.var=13, col="blue")
importance(heart_rf)

```

---

# Question 4: Boosting

- For boosting use `gbm` with ``cv.folds=5`` to perform 5-fold cross-validation, and set ``class.stratify.cv`` to ``AHD`` (heart disease outcome) so that cross-validation is performed stratifying by ``AHD``.  Plot the cross-validation error as a function of the boosting iteration/trees (the `$cv.error` component of the object returned by ``gbm``) and determine whether additional boosting iterations are warranted. If so, run additional iterations with  ``gbm.more`` (use the R help to check its syntax). Choose the optimal number of iterations. Use the ``summary.gbm`` function to generate the variable importance plot and extract variable importance/influence (``summary.gbm`` does both). Generate 1D and 2D marginal plots with ``gbm.plot`` to assess the effect of the top three variables and their 2-way interactions. 

```{r}


```
---


# Deliverables

1. Questions 1-4 answered, pdf or html output uploaded to quercus

https://github.com/annguyen1404/JSC370-labs
